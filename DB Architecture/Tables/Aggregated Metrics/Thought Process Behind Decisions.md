# Thought Process Behind Decisions

Designing the `aggregated_metrics` table required a **deliberate balance between performance, accuracy, and scalability**. Letâ€™s break this down into the **why behind each architectural choice**, especially with the perspective of impressing employers with thoughtful engineering design!

---

### **1. Why Hourly Summaries Instead of Raw Data?**

**Why It Matters:**

- Querying **raw server metrics at scale** is slow and expensive for dashboards and reports.
- **Pre-aggregating hourly metrics** reduces database load while maintaining **sufficient granularity** for performance monitoring.

**Thought Process:**

- I considered the **trade-off between granularity and performance** â€” while minute-by-minute data provides more precision, most operational insights (trends, averages, peaks) donâ€™t require that level of detail beyond the most recent timeframe.
- Storing **hourly averages, peaks, and uptime percentages** allows for **fast analytical queries** without sacrificing data accuracy for key performance metrics.
- Real-time data (last 5-10 minutes) is queried from the raw `server_metrics` table, while historical trends rely on this aggregated table.

âœ… **Why I Chose This Approach:**

- Avoids expensive real-time aggregations on raw data.
- Supports time-series analysis without impacting live monitoring performance.
- Perfect for dashboards where near-instant responsiveness is required.

---

### **2. Why Include Peak Metrics Alongside Averages?**

**Why It Matters:**

- Averages can hide important outliers, while peaks reveal **short-lived spikes** (e.g., CPU bursts, DDoS attacks, disk I/O bottlenecks).
- Knowing the **highest resource usage within an hour** allows for both **proactive scaling** and **performance anomaly detection**.

**Thought Process:**

- I deliberately added `peak_network_usage` and `peak_disk_usage` alongside averages, even though they require more storage.
- Employers care about **reliability engineering**, and knowing the highest load within a window helps prevent **silent SLA breaches**.
- Instead of calculating these peaks dynamically on each query, pre-storing them reduces query complexity and speeds up **real-time alerting systems**.

âœ… **Why I Chose This Approach:**

- Ensures critical system spikes arenâ€™t hidden behind averages.
- Provides clear insights for capacity planning and auto-scaling.
- Helps teams correlate traffic spikes with incident logs or performance degradation.

---

### **3. Why Track Uptime Percentage Instead of Raw Downtime?**

**Why It Matters:**

- Uptime percentage gives a clearer view of **service reliability** over a specific interval, which is essential for **SLA compliance**.
- Tracking just downtime events could require additional joins and complex logic to calculate uptime over arbitrary periods.

**Thought Process:**

- Rather than calculating uptime dynamically from `downtime_logs`, storing it as a precomputed value simplifies **dashboard reporting** and **alerting queries**.
- I considered calculating both downtime and uptime, but uptime percentage is a more **universal metric** for system reliability.
- In SLA-driven environments, comparing uptime percentages across servers, regions, or time periods is faster with this design.

âœ… **Why I Chose This Approach:**

- Reduces query complexity for uptime monitoring.
- Supports quick filtering for **SLA violations** and incident retrospectives.
- Keeps the metrics table self-contained without unnecessary joins.

---

### **4. Why Use DECIMAL for Resource Utilization Percentages?**

**Why It Matters:**

- Percentages for CPU, memory, and uptime need **precision without excessive storage overhead**.
- Floating-point types (e.g., FLOAT or REAL) can introduce rounding errors over time, especially in financial and performance calculations.

**Thought Process:**

- I opted for `DECIMAL(5,2)` instead of `FLOAT` because it offers **fixed-point precision**, which is critical for **accurate trend analysis** and SLA calculations.
- Employers expect attention to detail with data accuracy, and this shows a **strong understanding of precision storage choices** in relational databases.

âœ… **Why I Chose This Approach:**

- Avoids cumulative floating-point precision errors.
- Keeps storage efficient while allowing accurate percentage-based reporting.
- Supports standardized financial and performance calculations.

---

### **5. Why Region as a Column Instead of a Separate Lookup Table?**

**Why It Matters:**

- Normalizing regions into a separate table adds unnecessary joins for **performance-critical queries**, where region is often a filtering criterion.
- Since regions are a small, fixed dataset, denormalizing them into this table improves query performance.

**Thought Process:**

- I considered creating a `regions` lookup table with region IDs but realized that **minimizing joins on high-volume metrics queries** is more important than strict normalization here.
- Modern PostgreSQL storage optimization makes storing a few extra VARCHARs negligible compared to the performance gains from avoiding frequent joins.

âœ… **Why I Chose This Approach:**

- Prioritizes query speed over rigid normalization.
- Simplifies filtering and aggregation queries by region.
- Keeps the schema intuitive for quick analysis.

---

### **6. Scalability & Historical Analysis Considerations**

**Why It Matters:**

- Aggregated metric tables often become massive over time, impacting query performance and storage costs.
- A well-thought-out retention strategy ensures the table stays **lightweight and fast**, even with millions of rows.

**Thought Process:**

- I designed this table to support **partitioning by region and timestamp**, which allows fast queries for regional or time-based trends.
- Long-term data (beyond 30 days) is moved to **cold storage (S3, data lake)**, while only recent, relevant data remains in the active PostgreSQL database.
- For historical analysis, **daily summary tables** are created from hourly data to reduce row counts without losing key trends.

âœ… **Why I Chose This Approach:**

- Supports fast rollups for weekly, monthly, and quarterly reports.
- Keeps operational queries fast without affecting long-term trend analysis.
- Balances real-time performance monitoring with cost-effective data retention.

---

### **7. Automation & Alerting Decisions**

**Why It Matters:**

- Real-time alerting on `uptime_percentage`, `hourly_avg_cpu_usage`, and `peak_network_usage` allows teams to respond proactively before systems fail.
- Automating scaling decisions ensures the infrastructure can adjust dynamically to changing load patterns.

**Thought Process:**

- I designed triggers on this table to **notify incident response teams** if uptime falls below 99.9% or CPU usage exceeds 80%.
- Rather than manually monitoring metrics, this automation ensures **immediate visibility** into critical infrastructure issues.

âœ… **Why I Chose This Approach:**

- Enables proactive incident management instead of reactive troubleshooting.
- Reduces operational overhead while improving service reliability.
- Shows employers my ability to **think beyond storage** and design for **system resilience**.

---

## **Final Takeaways â€” Thought Process That Stands Out**

âœ… **Performance First:** Hourly aggregation, peak metrics, and uptime percentages support fast queries for real-time dashboards and historical analysis.

âœ… **Accuracy Matters:** DECIMAL for percentages, precomputed uptime, and cross-checks against raw server data ensure the metrics are precise and actionable.

âœ… **Scalability Baked In:** Partitioning, retention strategies, and cold storage integration prevent database bloat and support long-term analysis without slowing down active monitoring.

âœ… **Automation Ready:** Alert triggers and auto-scaling hooks turn aggregated data into a **proactive infrastructure management tool**, not just a static reporting table.

Would you like to dive deeper into **how these decisions create a competitive advantage** in system design interviews? Let me know if you want to build **interactive visualizations** with this aggregated data! ðŸš€âœ¨